# -*- coding: utf-8 -*-
"""Analyzing Mastdon Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xHtpraawF0TpT5VQaINUbbuehtMjfF9L

# **Task 1**: Create similarity network
Loads and cleans data to create a colored undirected weight graph to display activities between instances
"""

#Import all necessary packages
import re
import matplotlib.pyplot as plt
import nltk
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import scipy.sparse as sp
from collections import Counter, defaultdict

nltk.download('punkt')

# Example extended stopwords
stop_words = [
    'i','me','my','myself','we','our','ours',
    'ourselves','you',"you're","you've","you'll","you'd",'your','yours','yourself',
    'yourselves','he','him','his','himself','she',"she's",'her','hers','herself',
    'it',"it's",'its','itself','they','them','their','theirs','themselves',
    'what','which','who','whom','this','that',"that'll",'these','those','am','is','are',
    'was','were','be','been','being','have','has','had','having','do','does','did','doing',
    'a','an','the','and','but','if','or','because','as','until','while','of','at','by','for',
    'with','about','against','between','into','through','during','before','after','above','below','to',
    'from','up','down','in','out','on','off','over','under','again','further','then','once',
    'here','there','when','where','why','how','all','any','both','each','few','more','most',
    'other','some','such','no','nor','not','only','own','same','so','than','too','very',
    's','t','can','will','just','don',"don't",'should',"should've",'now','d','ll','m','o','re','ve','y',
    'ain','aren',"aren't",'couldn',"couldn't",'didn',"didn't",'doesn',"doesn't",
    'hadn',"hadn't",'hasn',"hasn't",'haven',"haven't",'isn',"isn't",'ma','mightn',
    "mightn't",'mustn',"mustn't",'needn',"needn't",'shan',"shan't",'shouldn',"shouldn't",
    'wasn',"wasn't",'weren',"weren't",'won',"won't",'wouldn',"wouldn't"
]

class ModerationData:
    def __init__(self, file_name):
        self.file_name = file_name
        # placeholders
        self.df = None
        self.node_community_map = {}
        self.sorted_communities = []

    # Clean & tokenize
    def clean_and_tokenize(self, text):
        # Convert text to lowercase
        text = text.lower()

        # Remove URLs
        text = re.sub(r"http\S+|www\S+|t\.co/\S+", "", text)

        # Remove special characters and numbers
        text = re.sub(r"[^a-zA-Z\s]", '', text)

        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text).strip()

        # Tokenize the cleaned text
        tokens = nltk.word_tokenize(text)

        # Remove stopwords
        tokens = [word for word in tokens if word not in stop_words]

        return tokens

    # Load dataset & apply cleaning
    def load_data_and_apply_cleaning(self):
        """Loads the CSV, cleans 'blocking_reason' column."""
        self.df = pd.read_csv(self.file_name)
        # Convert 'source' and 'target' to string to remove int64 representation in later functions
        self.df['source'] = self.df['source'].astype(str)
        self.df['target'] = self.df['target'].astype(str)
        # Clean blocking_reason by applying the tokenize function
        self.df['blocking_reason'] = self.df['blocking_reason'].apply(self.clean_and_tokenize)
        return self.df

    # Filter out blank blocking reasons
    def filter_blank_entries(self):
        self.df = self.df.dropna(subset=['blocking_reason'])
        return self.df

    # Filter the data to only show reasons given by the user
    def filter_by_reasons(self, reasons):
        filtered_df = self.df[self.df['blocking_reason'].isin(reasons)]
        return filtered_df

    # Create a pivot table & sparse matrix
    def create_sparse_matrix(self):
        # Create a pivot table with target=rows, source= columns and the values in the boxes being the weight column of the source,target instance
        pivot_df = self.df.pivot_table(index='target', columns='source', values='weight', fill_value=0)
        # Create a sparse matrix from the pivot table
        sparse_matrix = csr_matrix(pivot_df)
        print(f"Sparse Matrix Created! Shape: {sparse_matrix.shape}")
        return sparse_matrix

    # Calculate Cosine Similarity among "source" nodes
    def calculate_cosine_similarity(self, sparse_matrix):
        """
        Computes cosine similarity between source nodes (moderators/sources).
        Returns a DataFrame of similarity values.
        """
        similarity_matrix = cosine_similarity(sparse_matrix.T)
        # Sorts only unique users to get rid of repitition
        moderators = self.df["source"].unique()
        similarity_df = pd.DataFrame(similarity_matrix, index=moderators, columns=moderators)
        print("Cosine Similarity Matrix (first 5x5 snippet):\n", similarity_df.iloc[:5, :5])
        return similarity_df

    # Build an undirected weighted graph from the similarity DataFrame
    def undirected_weight_graph(self, similarity_df):
        G = nx.Graph()
        # Add nodes
        for source in similarity_df.index:
            G.add_node(source)

        # Add edges for nonzero similarities
        for source1 in similarity_df.index:
          edges= 0
          for source2 in similarity_df.index:
             if source1 != source2: # Ensures that nodes aren't connected to themselves
                weight_val = similarity_df.loc[source1, source2]
                if weight_val > 0.0:
                  G.add_edge(source1, source2, weight=weight_val)
                  edges+=1
          # Gets rid of outliers nodes without any edges
          if edges ==0:
            G.remove_node(source1)
        edges = G.edges(data=True)
        # Match the corresponding weight for the edges from the weight column
        weights = [edge[2]['weight'] for edge in edges]
        #Create a threshold of edges to keep(ie. must be above the 90th percentile of edge weights)
        threshold = np.percentile(weights, 90)
        # Only keeps the top 90% of edges
        edges_to_keep = [(u, v) for (u, v, w) in edges if w['weight'] >= threshold]
        return G

    # Color the nodes by 'blocking_reason'
    def color_nodes_by_attribute(self, G):
        attribute_map = {}
        for mod in G.nodes:
            # Find the first row in df with the same source
            subset = self.df[self.df['source'] == mod]
            # Get blocking_reason
            reason_list = subset.iloc[0]['blocking_reason']
            # Convert to string
            reason_str = ' '.join(reason_list) if isinstance(reason_list, list) else str(reason_list)
            attribute_map[mod] = reason_str
        # Assign colors to each blocking reason
        color_mapping = {
            "harassment": 'red',
            "spam": 'blue',
            "misinformation": 'black',
            "hatespeech": 'green'
        }

        node_colors = []
        for n in G.nodes():
            reason_str = attribute_map[n]
            node_colors.append(color_mapping.get(reason_str, "grey"))

        plt.figure(figsize=(8, 6))
        pos = nx.spring_layout(G,)
        nx.draw(G, pos, with_labels=False, node_color=node_colors, edge_color="gray", width=2)
        plt.title("Moderator Similarity Network (Colored by Attribute)")
        # Create legend elements for each blocking reason
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', label=key, markerfacecolor=val, markersize=10)
            for key, val in color_mapping.items()
        ]
        plt.legend(handles=legend_elements, loc="upper right", title="Blocking Reason")
        plt.axis('off')
        plt.show()

# Create a ModerationData object
moderation_data = ModerationData('mastodon_edgelist_AE2.csv')

# Load the data
moderation_data.load_data_and_apply_cleaning()
# Filter blank entries
moderation_data.filter_blank_entries()

# Filter by specific reasons
reasons = ['Misinformation', 'Spam', 'Harassment', 'Hate-Speech']
filtered_data = moderation_data.filter_by_reasons(reasons)
# Create the sparse matrix
sparse_matrix = moderation_data.create_sparse_matrix()
cosine_similarity_matrix = moderation_data.calculate_cosine_similarity(sparse_matrix)
G = moderation_data.undirected_weight_graph(cosine_similarity_matrix)
moderation_data.color_nodes_by_attribute(G)

"""# Analyzing the Moderator Similarity Graph:
The Colored Moderator Similarity Network visualizes that moderators who block misinformation or harassment often stick to mainly blocking misinformation and harassment posts. This makese sense as a lot of harassment could fall under misinformation and if a moderator was looking for those types of hateful posts he would take down misinformation and harassement posts. Addtionally, moderators who block hate-speech posts often also block misinformation, implying that these moderators take down posts like fake news aiming at tarnishing someones reputation. Finally, moderators who take down spam posts often take down a variety of other posts. This could mean that moderators don't simply look for spam posts but instead come across them while they are looking for other types of posts.

# **Task 2: Identify and visualize the communities by performing community detection**
Displays the connectedness of different communities, grouped on their moderation activities, and the distribution of the communities based on instances and blocking reasons.
"""

#Create another class that takes Moderation Data as a subclass and passes it
class Community_Analyzing(ModerationData):
    def __init__(self, file_name):
        super().__init__(file_name)
        self.node_community_map = {}
 # Louvain Community Detection and Visualization
    def community_detection(self, G):
        # Perform Louvain community detection
        communities = nx.community.louvain_communities(G)

        # Assign each node to a community
        self.node_community_map = {}
        for i, comm in enumerate(communities):
            for node in comm:
                self.node_community_map[node] = i

        # Count the number of nodes in each community
        community_sizes = Counter(self.node_community_map.values())

        # Define threshold: exclude communities with fewer than 3 nodes
        threshold = 2
        filtered_communities = {k: v for k, v in community_sizes.items() if v >= threshold}

        # Sort the filtered communities by size and reassign IDs
        self.sorted_communities = sorted(filtered_communities.items(), key=lambda x: x[1], reverse=True)
        reassigned_community_map = {old_id: new_id for new_id, (old_id, _) in enumerate(self.sorted_communities, start=1)}

        # Color mapping for remaining communities
        num_filtered_comms = len(self.sorted_communities)
        community_colors = plt.cm.viridis(np.linspace(0, 1, num_filtered_comms))

        # Node color assignment
        node_colors_community = []
        for n in G.nodes():
            old_comm_id = self.node_community_map[n]
            if old_comm_id in filtered_communities:
                new_comm_id = reassigned_community_map[old_comm_id]
                node_colors_community.append(community_colors[new_comm_id - 1])
            else:
                node_colors_community.append("gray")

        # Layout
        pos_sim = nx.spring_layout(G,)
        plt.figure(figsize=(10, 8))
        nx.draw_networkx_nodes(G, pos_sim, node_color=node_colors_community, node_size=50, alpha=0.8)
        edge_widths = [G[u][v].get('weight', 1.0)*5 for u, v in G.edges()]
        nx.draw_networkx_edges(G, pos_sim, width=edge_widths, alpha=0.5)

        plt.title("Louvain Community Detection")
        plt.axis('off')
        plt.show()
        # Ensure all moderators are in communities
    # Summarize Community Info by bar graph
    def summarize_community_info(self):
      community_summary = {}

      # Initialize a dictionary for each filtered community from sorted_communities
      for self.community_id, comm_size in self.sorted_communities:
        community_summary[self.community_id] = {
            'community_size': comm_size,
            'instances': [],
            'group_counts': defaultdict(int)
        }

      # Convert self.sorted_communities to a dict
      community_dict = {c_id: size for c_id, size in self.sorted_communities}

      # Get all unique 'source' noes
      all_sources = self.df['source'].unique()

      # For each source see if it's in node_community_map
      for source in all_sources:
        if source in self.node_community_map:
            old_comm_id = self.node_community_map[source]

            # Check if old_comm_id is in our filtered communities
            if old_comm_id in community_dict:
              self.community_id = old_comm_id
              community_summary[self.community_id]['instances'].append(source)

              # Find the blocking_reason for this source
              subset = self.df[self.df['source'] == source]
              reason_list = subset.iloc[0]['blocking_reason']
              # Join reason list into a string
              reason_str = ' '.join(reason_list) if isinstance(reason_list, list) else str(reason_list)
              community_summary[self.community_id]['group_counts'][reason_str] += 1

      # Print results
      for self.community_id, info in community_summary.items():
        print(f"Community {self.community_id}:")
        print(f"  Size: {info['community_size']}")
        print(f"  Instances: {info['instances']}")
        group_counts_dict = dict(info['group_counts'])
        print(f"  Group Counts: {group_counts_dict}")
        print()

    def community_count_graphs(self):
      categories = list(self.node_community_map.values())
      counts = Counter(categories)
      print("Counts(From Highest to Lowest):", counts)  # Print the counts


      plt.bar(counts.keys(), counts.values())
      plt.xlabel('Category')
      plt.ylabel('Count')
      plt.title('Category Counts')
      plt.show()
#Load the Data
Community_Analyzing = Community_Analyzing('mastodon_edgelist_AE2.csv')
#Use Moderation Data functions to ready data
Community_Analyzing.load_data_and_apply_cleaning()
Community_Analyzing.filter_blank_entries()
#Apply Community Analyzing functions
Community_Analyzing.community_detection(G)
Community_Analyzing.summarize_community_info()
Community_Analyzing.community_count_graphs()
node_community_map = Community_Analyzing.node_community_map
sorted_communities = Community_Analyzing.sorted_communities

"""# Analysis of the distribution of instances across the communities:
The Louvain Algorithm and Category Bar graph display that the two largest communities—1 and 3—were primarily composed of instances involving harassment and misinformation, with instance counts of 10 and 8, respectively. Similar to the Moderator Similarity Graphs, this further supports the idea that certain moderators are assigned to remove bullying‐type content. Community 3, with an instance count of 8, included all four types of blocking reasons, suggesting that a few moderators were assigned more than one type of post to take down. Spam posts were never the majority of any community's instances, once again supporting the idea that moderators do not simply look for spam but instead come across it while looking for other types of content.

#  Task 3: Reflecting on the process:
For task 1, the first problem I ran into was figuring out how to apply the cleaning and processing to the text. I struggled with finding an efficient way to convert the column into text and process the "Blocking_Reason" column. I concluded that creating a separate function that cleans text that is passed through it and running that function on the 'Blocking_Reason' column would be the easiest. The issue I ran into was creating a meaningful moderation similarity graph. At first, my graph was largely disconnected with only a half of the nodes in the center with the other half being on the sides without edges. I tried to higher the threshold of edge weights displayed, but instead of helping, this actually made the graph more dissconnected. I then tried to make the threshold lower, making more of the nodes connected but still leaving a lot of outliers.Next, I tried to make a new threshold of only the top 90% of edges,in terms of weight. This helped my graph a bit more, but still left around 10 outliers. Finally, I decided to remove all nodes without edges, removing all outliers.

For task 2, one of the initial challenges was integrating the Louvain community detection with our dataset. I struggled with ensuring that each moderator was correctly assigned a community, especially since the node IDs were originally stored as int64, which made the outputs look messy. I resolved this by converting the IDs to strings immediately after loading the data. Next, I ran into difficulties summarizing the community information.I first tried using the next() function to match community IDs from the filtered list, but that approach was error-prone when community sizes were small or when some nodes weren't assigned properly. I switched to a dictionary-based lookup, which made the logic clearer and ensured that each source's community was accurately recorded. Additionally, setting a meaningful threshold for filtering out small communities was tricky, the graph was either too sparse or too cluttered. After experimenting with different thresholds, I settled on excluding communities with fewer than three nodes.

For task 4, I wanted to analyze if moderators were assigned to take down posts from certain users or if they collaborate and take down the same users. I looked to graph the amount of different communities these moderators came from and the blocking reasons for these posts. One of the biggest problems I had was trying to send the node community map from my community analysis class to my new class for task 4. I wasn't able to intially implement the map to match the moderator id's with their communities, leaving the communities blank. I made sure that all moderators were assigned to a community by giving them a default community of Unknown if they weren't already sorted. After a lot of trouble-shooting I found out that, suprisingly, all of the moderators were not sorted into a community, essentially rendering my 'posts taken down from different numbers of communities' data useless.

# **Task 4: Going above and beyond**
To further analyze the dataset, I chose to find any mastadon users whose posts were taken down by several moderators. I will commare what their posts were taken down for and make a claim about repeat offenders.
"""

# Load the dataset
df = pd.read_csv('mastodon_edgelist_AE2.csv')

# Define the Repeat_Offenses class that inherits from ModerationData
class Repeat_Offenses(ModerationData):
    def __init__(self, file_name, Community_Analyzing_object):
        # Initialize the parent class with the file_name
        super().__init__(file_name)
        # Set the community map from the passed object
        self.node_community_map = Community_Analyzing_object.node_community_map


    def Graphing_Repeat_Offenses(self):
        target_counts = df['target'].value_counts()
        # Filter users who were blocked more than once
        repeated_users = target_counts[target_counts > 1]
        # Filter the dataframe for repeat offenders
        df.repeat_offenses = df[df['target'].isin(repeated_users.index)]
        # Count the blocking reasons for repeat offenders
        reason_counts = df.repeat_offenses['blocking_reason'].value_counts()


        plt.figure(figsize=(8, 6))
        reason_counts.plot(kind='bar', color='skyblue')
        plt.title("Blocking Reasons Among Repeat Offenders")
        plt.xlabel("Blocking Reason")
        plt.ylabel("Frequency")
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()


    def Analyzing_Moderators_of_Repeat_Offenders(self):

      blocks_made_from_one_community = 0
      blocks_made_from_two_communities = 0
      blocks_made_from_three_or_more_communities = 0
      all_results = []  # List to store the results for printing later

      # Ensure that every moderator has an associated community
      for mod in df['source'].unique():
            if mod not in self.node_community_map:
                # Assign 'Unknown' as the default community if missing
                self.node_community_map[mod] = 'Unknown'

      # Group by target (user being blocked)
      for user_id, subdf in df.repeat_offenses.groupby('target'):
        # Find unique moderators who blocked this user
        mods = subdf['source'].unique()
        mod_communities = []

        # Add each moderators community to the list
        for m in mods:
          comm_id = self.node_community_map.get(m,'blank')
          mod_communities.append(comm_id)

        # Get the number of unique communities involved in blocking this user
        unique_communities = set(mod_communities)
        # Count the number of unique communities
        num_unique_communities = len(unique_communities)

        # Update the block counters based on the number of unique communities
        if num_unique_communities == 1:
          blocks_made_from_one_community += 1
        elif num_unique_communities == 2:
          blocks_made_from_two_communities += 1
        else:
          blocks_made_from_three_or_more_communities += 1


        result = f"User '{user_id}' was blocked {len(subdf)} times by {len(mods)} moderators.\n"
        result += f"  Moderators: {mods.tolist()}\n"
        result += f"  Communities: {mod_communities}\n\n"
        all_results.append(result)

      # Print only the first 5 results
      for result in all_results[:5]:
        print(result)


      print(f"Total Blocks from One Community: {blocks_made_from_one_community}")
      print(f"Total Blocks from Two Communities: {blocks_made_from_two_communities}")
      print(f"Total Blocks from Three or More Communities: {blocks_made_from_three_or_more_communities}")


repeat_offenses = Repeat_Offenses("mastodon_edgelist_AE2.csv", Community_Analyzing)
repeat_offenses.load_data_and_apply_cleaning()
repeat_offenses.filter_blank_entries()

# Call the methods to graph
repeat_offenses.Graphing_Repeat_Offenses()
repeat_offenses.Analyzing_Moderators_of_Repeat_Offenders()

"""# **Repeat Offenses Analysis:**
The users who were blocked by multiple moderators seem to post misinformation the most, with around 210 users, while harassment had the least with 175 users. This slight difference could be attributed to the moderators giving more leeway to misinformation posts as it can take a lot to classify a post as misinformation in comparison to classifying a post as harassment. Additionally, this could mean that if a user were to post something including harassment they would be given less chances before they were removed from the cite. Unfortunately all of the moderators that were involved in these take downs were not assigned to communities leaving the blocks from different numbers of communities statistics useless. This could be caused by moderators lack of consistent pattern with the posts they take down.
"""